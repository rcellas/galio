from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.firefox.options import Options
import time

def scrape_multiple_websites(urls, keywords):
    # Configuraci√≥n del driver de Selenium con opciones
    options = Options()
    options.add_argument('--headless')  # Ejecutar en modo sin interfaz gr√°fica
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    # Especificar la ruta del geckodriver
    service = Service("/usr/local/bin/geckodriver")
    driver = webdriver.Firefox(service=service, options=options)

    scraped_data = []  # Lista para almacenar los datos extra√≠dos

    try:
        for url in urls:
            print(f"üîé Scraping URL: {url}")
            driver.get(url)  # Cargar la URL en el navegador

            # Esperar hasta que el cuerpo de la p√°gina est√© presente
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # Buscar iframes dentro de la p√°gina
            iframes = driver.find_elements(By.TAG_NAME, "iframe")
            if iframes:
                print(f"üîÑ Se encontraron {len(iframes)} iframes. Cambiando al primero.")
                driver.switch_to.frame(iframes[0])  # Cambiar al primer iframe
                
                # Esperar que el contenido del iframe cargue completamente
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )

            # Agregar un retraso extra para asegurar la carga completa de la p√°gina
            time.sleep(5)

            # Buscar los elementos con la clase espec√≠fica que contiene la informaci√≥n
            sections = driver.find_elements(By.CLASS_NAME, "imc--llistat")

            # Procesar los elementos encontrados y filtrar por palabras clave
            for section in sections:
                lines = section.text.strip().split("\n")  # Separar el texto en l√≠neas
                for line in lines:
                    if any(kw.lower() in line.lower() for kw in keywords):
                        scraped_data.append({"text": line.strip()})  # Agregar la l√≠nea coincidente

    except Exception as e:
        print("‚ùå Error al procesar las URLs:", e)
    finally:
        driver.quit()  # Cerrar el navegador al finalizar

    print("‚úÖ Scraped Data:", scraped_data)  # Mostrar los datos extra√≠dos
    return scraped_data

# Definir URLs a analizar y palabras clave para filtrar
urls = ["https://dogv.gva.es/es/inici"]
keywords = ["subvenci√≥n", "subvenciones","SUBVENCIONES" "licitaci√≥n", "contrato", "contratos"]

# Ejecutar la funci√≥n de scraping
scraped_data = scrape_multiple_websites(urls, keywords)
